#!/bin/bash -l

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=2G
#SBATCH --job-name=pdb_2_ko
#SBATCH --output=pdb_2_ko.out
#SBATCH --error=pdb_2_ko.err

#source ~/.bashrc
# conda deactivate
conda activate bif2021

# This job creates a table for a subset of "hypotetical proteins" from prokka, but also 
# with no KO annotations from eggnog-mapper. The input is a directory of hhr files, produced
# by a run of hhblits using the "pdb70" DB against these "hypotetical proteins" (check hhsuite wiki).
# Dependencies: Apart from python >= 3.6, the second script needs solrq, which can be installed with pip
# (Not available in conda)i and bioservices (bioconda)

echo `date`
SECONDS=0
py_hhblits="/home/jmaturana/Software/cli_tools/hypothetical_proteins/hhblits_select_pdbs.py"
py_pdb_to_kegg="/home/jmaturana/Software/cli_tools/hypothetical_proteins/pdb_to_kegg.py"

in_hhr="/mnt/DATA/jmaturana/Hypothetical_proteins/hhrs"
out_dir="/mnt/DATA/jmaturana/Hypothetical_proteins"

python $py_hhblits $in_hhr $out_dir

# The next step uses the Uniprot and KEGG API, so it takes a few seconds per query/gene. 
# The previous script creates "high_prob.pkl" in the $out_dir and will be used as input for the step below
python $py_pdb_to_kegg "${out_dir}/high_prob.pkl" "${out_dir}/pdb_300k_uni_ko.tsv"

duration=$SECONDS
echo "$(($duration / 60)) minutes and $(($duration % 60)) seconds elapsed."
