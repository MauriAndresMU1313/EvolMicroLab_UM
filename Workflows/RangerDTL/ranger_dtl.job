#!/bin/bash -l

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=28
#SBATCH --mem=20G
#SBATCH --job-name=rangerdtl
#SBATCH --output=ranger_dtl.out
#SBATCH --error=ranger_dtl.err

# This was run in darwin

source ~/.bashrc
conda deactivate
export PATH="/home/jmaturana/miniconda3/envs/bif2021.1/bin:$PATH"


### edit species input_tree ###
root="${HOME}/Akker/g_akk_prokka_faa_repr_fast/OrthoFinder"
orth_results="Results_Nov25"
py_edit_tree="${HOME}/scripts/cli_tools/RangerDTL/edit_species_tree.py"
sp_tree="${root}/${orth_results}/Species_Tree/SpeciesTree_rooted_node_labels.txt"
orth_results="Results_Nov25"

# Go inside the particular orthofinder results directory
cd $root/$orth_results
# Edited trees will go into Species_Tree/
python $py_edit_tree $sp_tree Species_Tree/
# output: Species_Tree/species_tree_edited_rand_N0.nwk Species_Tree/sname_to_rand.pkl

### Select trees for each single-copy OG ###

# already inside orthofinder results directory
mkdir single_copy_trees
parallel  "cp Resolved_Gene_Trees/{/.}_tree.txt single_copy_trees" ::: Single_Copy_Orthologue_Sequences/*


### Run the ranger pipeline ###

echo `date`
SECONDS=0

node_to_use="N1"
py_ranger="${HOME}/scripts/cli_tools/RangerDTL/ranger-dtl_pipeline.py"
# From $edit_sp
edited_stree="${root}/${orth_results}/Species_Tree/species_tree_edited_rand_N0.nwk"
sname_dict="${root}/${orth_results}/Species_Tree/sname_to_rand.pkl"
# from $py_create
input_trees="${root}/${orth_results}/single_copy_trees"
# The output files will be here:
output_dir="${root}/${orth_results}/RangerDTL"
# number of seeds: 100-500
n_seeds=200
# Ranger binaries
RangerDTL="${HOME}/Software/RangerDTL_Linux/CorePrograms/Ranger-DTL.linux"
AggregateRanger="${HOME}/Software/RangerDTL_Linux/CorePrograms/AggregateRanger.linux"

# It should be run with gnu parallel because iterates 100s of times over 100+ HOGs
ncpus=$SLURM_CPUS_PER_TASK
parallel -j $ncpus "python $py_ranger $edited_stree {} $sname_dict $n_seeds ${output_dir} \
 $RangerDTL $AggregateRanger" ::: ${input_trees}/*.txt

wait

#  AggregateRanger dir is hardcoded into ranger-dtl_pipeline.py
aggregated_dir="${output_dir}/AggregateRanger"
py_agg="/home/jmaturana/scripts/cli_tools/RangerDTL/aggregate_aggs.py"
# Takes all the aggregated files and creates a single summary table
# Needs the $n_seeds because the number of runs/files matter for the stats
python $py_agg $aggregated_dir $sname_dict $n_seeds
# ${output_dir}/nodes_events.tsv should be the final table

tar -cf ranger_dtl_out.tar.gz $edited_stree $sname_dict $aggregated_dir ${output_dir}/nodes_events.tsv

duration=$SECONDS
echo "$(($duration / 60)) minutes and $(($duration % 60)) seconds elapsed."
echo `date`
